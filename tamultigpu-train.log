2019-09-12 01:34:01,991 [train.py:29] INFO Configurations: Namespace(batch_size=32, data_dir='/home/comp/15485625/data/speech/sp2chs', datasets='thchs30,aishell', epochs=100, log='tamultigpu-train.log', logprefix='tamultigpu', lr=0.0008, nworkers=4, pretrain=None, saved_dir='./checkpoint')
2019-09-12 01:35:45,520 [deprecation.py:323] WARNING From /home/comp/15485625/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-09-12 01:35:53,129 [deprecation.py:506] WARNING From /home/comp/15485625/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-09-12 01:35:53,211 [layer_utils.py:106] INFO Model: "model_1"
2019-09-12 01:35:53,211 [layer_utils.py:107] INFO _________________________________________________________________
2019-09-12 01:35:53,211 [layer_utils.py:104] INFO Layer (type)                 Output Shape              Param #   
2019-09-12 01:35:53,211 [layer_utils.py:109] INFO =================================================================
2019-09-12 01:35:53,211 [layer_utils.py:104] INFO the_inputs (InputLayer)      (None, None, 200, 1)      0         
2019-09-12 01:35:53,211 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,212 [layer_utils.py:104] INFO conv2d_1 (Conv2D)            (None, None, 200, 32)     320       
2019-09-12 01:35:53,212 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,212 [layer_utils.py:104] INFO batch_normalization_1 (Batch (None, None, 200, 32)     128       
2019-09-12 01:35:53,212 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,212 [layer_utils.py:104] INFO conv2d_2 (Conv2D)            (None, None, 200, 32)     9248      
2019-09-12 01:35:53,212 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,212 [layer_utils.py:104] INFO batch_normalization_2 (Batch (None, None, 200, 32)     128       
2019-09-12 01:35:53,212 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,212 [layer_utils.py:104] INFO max_pooling2d_1 (MaxPooling2 (None, None, 100, 32)     0         
2019-09-12 01:35:53,212 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,212 [layer_utils.py:104] INFO conv2d_3 (Conv2D)            (None, None, 100, 64)     18496     
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,213 [layer_utils.py:104] INFO batch_normalization_3 (Batch (None, None, 100, 64)     256       
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,213 [layer_utils.py:104] INFO conv2d_4 (Conv2D)            (None, None, 100, 64)     36928     
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,213 [layer_utils.py:104] INFO batch_normalization_4 (Batch (None, None, 100, 64)     256       
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,213 [layer_utils.py:104] INFO max_pooling2d_2 (MaxPooling2 (None, None, 50, 64)      0         
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,213 [layer_utils.py:104] INFO conv2d_5 (Conv2D)            (None, None, 50, 128)     73856     
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,213 [layer_utils.py:104] INFO batch_normalization_5 (Batch (None, None, 50, 128)     512       
2019-09-12 01:35:53,213 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,214 [layer_utils.py:104] INFO conv2d_6 (Conv2D)            (None, None, 50, 128)     147584    
2019-09-12 01:35:53,214 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,214 [layer_utils.py:104] INFO batch_normalization_6 (Batch (None, None, 50, 128)     512       
2019-09-12 01:35:53,214 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,214 [layer_utils.py:104] INFO max_pooling2d_3 (MaxPooling2 (None, None, 25, 128)     0         
2019-09-12 01:35:53,214 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,214 [layer_utils.py:104] INFO conv2d_7 (Conv2D)            (None, None, 25, 128)     147584    
2019-09-12 01:35:53,214 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,214 [layer_utils.py:104] INFO batch_normalization_7 (Batch (None, None, 25, 128)     512       
2019-09-12 01:35:53,214 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,214 [layer_utils.py:104] INFO conv2d_8 (Conv2D)            (None, None, 25, 128)     147584    
2019-09-12 01:35:53,214 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,215 [layer_utils.py:104] INFO batch_normalization_8 (Batch (None, None, 25, 128)     512       
2019-09-12 01:35:53,215 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,215 [layer_utils.py:104] INFO conv2d_9 (Conv2D)            (None, None, 25, 128)     147584    
2019-09-12 01:35:53,215 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,215 [layer_utils.py:104] INFO batch_normalization_9 (Batch (None, None, 25, 128)     512       
2019-09-12 01:35:53,215 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,215 [layer_utils.py:104] INFO conv2d_10 (Conv2D)           (None, None, 25, 128)     147584    
2019-09-12 01:35:53,215 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,215 [layer_utils.py:104] INFO batch_normalization_10 (Batc (None, None, 25, 128)     512       
2019-09-12 01:35:53,215 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,215 [layer_utils.py:104] INFO reshape_1 (Reshape)          (None, None, 3200)        0         
2019-09-12 01:35:53,215 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,216 [layer_utils.py:104] INFO dropout_1 (Dropout)          (None, None, 3200)        0         
2019-09-12 01:35:53,216 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,216 [layer_utils.py:104] INFO dense_1 (Dense)              (None, None, 256)         819456    
2019-09-12 01:35:53,216 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,216 [layer_utils.py:104] INFO dropout_2 (Dropout)          (None, None, 256)         0         
2019-09-12 01:35:53,216 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-12 01:35:53,216 [layer_utils.py:104] INFO dense_2 (Dense)              (None, None, 1214)        311998    
2019-09-12 01:35:53,216 [layer_utils.py:169] INFO =================================================================
2019-09-12 01:35:53,217 [layer_utils.py:182] INFO Total params: 2,012,062
2019-09-12 01:35:53,217 [layer_utils.py:183] INFO Trainable params: 2,010,142
2019-09-12 01:35:53,217 [layer_utils.py:184] INFO Non-trainable params: 1,920
2019-09-12 01:35:53,217 [layer_utils.py:185] INFO _________________________________________________________________
2019-09-12 01:35:57,805 [train.py:84] INFO # of samples: 130098
2019-09-12 01:35:57,823 [train.py:85] INFO mini-batch size: 32
2019-09-12 01:35:57,823 [train.py:86] INFO # of iterations per epoch: 4065
2019-09-12 01:35:57,878 [deprecation.py:323] WARNING From /home/comp/15485625/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-09-12 01:35:58,084 [deprecation.py:323] WARNING From /home/comp/15485625/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
2019-09-12 01:37:00,391 [callbacks.py:16] INFO For batch 40/4065, loss is  133.11.
2019-09-12 01:37:23,484 [callbacks.py:16] INFO For batch 80/4065, loss is  100.61.
2019-09-12 01:37:50,199 [callbacks.py:16] INFO For batch 120/4065, loss is  101.35.
2019-09-12 01:38:12,399 [callbacks.py:16] INFO For batch 160/4065, loss is   99.45.
2019-09-12 01:38:36,459 [callbacks.py:16] INFO For batch 200/4065, loss is  101.77.
2019-09-12 01:39:06,899 [callbacks.py:16] INFO For batch 240/4065, loss is   98.94.
2019-09-12 01:39:35,055 [callbacks.py:16] INFO For batch 280/4065, loss is   99.77.
2019-09-11 11:26:43,344 [train.py:31] INFO Configurations: Namespace(batch_size=64, data_dir='/datasets/shshi/speech/sp2chs', datasets='thchs30,aishell', epochs=100, log='tamultigpu-train.log', logprefix='tamultigpu', lr=0.0008, nworkers=8, pretrain=None, saved_dir='/datasets/shshi/checkpoint2')
2019-09-11 11:27:45,342 [deprecation.py:323] WARNING From /home/shaohuais/tf1.13.1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-09-11 11:27:59,857 [deprecation.py:506] WARNING From /home/shaohuais/tf1.13.1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-09-11 11:27:59,922 [layer_utils.py:106] INFO Model: "model_1"
2019-09-11 11:27:59,923 [layer_utils.py:107] INFO _________________________________________________________________
2019-09-11 11:27:59,923 [layer_utils.py:104] INFO Layer (type)                 Output Shape              Param #   
2019-09-11 11:27:59,924 [layer_utils.py:109] INFO =================================================================
2019-09-11 11:27:59,926 [layer_utils.py:104] INFO the_inputs (InputLayer)      (None, None, 200, 1)      0         
2019-09-11 11:27:59,926 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,927 [layer_utils.py:104] INFO conv2d_1 (Conv2D)            (None, None, 200, 32)     320       
2019-09-11 11:27:59,928 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,928 [layer_utils.py:104] INFO batch_normalization_1 (Batch (None, None, 200, 32)     128       
2019-09-11 11:27:59,929 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,930 [layer_utils.py:104] INFO conv2d_2 (Conv2D)            (None, None, 200, 32)     9248      
2019-09-11 11:27:59,931 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,932 [layer_utils.py:104] INFO batch_normalization_2 (Batch (None, None, 200, 32)     128       
2019-09-11 11:27:59,932 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,933 [layer_utils.py:104] INFO max_pooling2d_1 (MaxPooling2 (None, None, 100, 32)     0         
2019-09-11 11:27:59,934 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,935 [layer_utils.py:104] INFO conv2d_3 (Conv2D)            (None, None, 100, 64)     18496     
2019-09-11 11:27:59,936 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,937 [layer_utils.py:104] INFO batch_normalization_3 (Batch (None, None, 100, 64)     256       
2019-09-11 11:27:59,938 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,939 [layer_utils.py:104] INFO conv2d_4 (Conv2D)            (None, None, 100, 64)     36928     
2019-09-11 11:27:59,940 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,941 [layer_utils.py:104] INFO batch_normalization_4 (Batch (None, None, 100, 64)     256       
2019-09-11 11:27:59,941 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,942 [layer_utils.py:104] INFO max_pooling2d_2 (MaxPooling2 (None, None, 50, 64)      0         
2019-09-11 11:27:59,943 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,944 [layer_utils.py:104] INFO conv2d_5 (Conv2D)            (None, None, 50, 128)     73856     
2019-09-11 11:27:59,945 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,945 [layer_utils.py:104] INFO batch_normalization_5 (Batch (None, None, 50, 128)     512       
2019-09-11 11:27:59,946 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,947 [layer_utils.py:104] INFO conv2d_6 (Conv2D)            (None, None, 50, 128)     147584    
2019-09-11 11:27:59,948 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,948 [layer_utils.py:104] INFO batch_normalization_6 (Batch (None, None, 50, 128)     512       
2019-09-11 11:27:59,949 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,950 [layer_utils.py:104] INFO max_pooling2d_3 (MaxPooling2 (None, None, 25, 128)     0         
2019-09-11 11:27:59,951 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,952 [layer_utils.py:104] INFO conv2d_7 (Conv2D)            (None, None, 25, 128)     147584    
2019-09-11 11:27:59,953 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,954 [layer_utils.py:104] INFO batch_normalization_7 (Batch (None, None, 25, 128)     512       
2019-09-11 11:27:59,954 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,955 [layer_utils.py:104] INFO conv2d_8 (Conv2D)            (None, None, 25, 128)     147584    
2019-09-11 11:27:59,955 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,956 [layer_utils.py:104] INFO batch_normalization_8 (Batch (None, None, 25, 128)     512       
2019-09-11 11:27:59,957 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,958 [layer_utils.py:104] INFO conv2d_9 (Conv2D)            (None, None, 25, 128)     147584    
2019-09-11 11:27:59,959 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,960 [layer_utils.py:104] INFO batch_normalization_9 (Batch (None, None, 25, 128)     512       
2019-09-11 11:27:59,960 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,961 [layer_utils.py:104] INFO conv2d_10 (Conv2D)           (None, None, 25, 128)     147584    
2019-09-11 11:27:59,962 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,963 [layer_utils.py:104] INFO batch_normalization_10 (Batc (None, None, 25, 128)     512       
2019-09-11 11:27:59,964 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,964 [layer_utils.py:104] INFO reshape_1 (Reshape)          (None, None, 3200)        0         
2019-09-11 11:27:59,965 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,966 [layer_utils.py:104] INFO dropout_1 (Dropout)          (None, None, 3200)        0         
2019-09-11 11:27:59,966 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,967 [layer_utils.py:104] INFO dense_1 (Dense)              (None, None, 256)         819456    
2019-09-11 11:27:59,968 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,969 [layer_utils.py:104] INFO dropout_2 (Dropout)          (None, None, 256)         0         
2019-09-11 11:27:59,969 [layer_utils.py:171] INFO _________________________________________________________________
2019-09-11 11:27:59,970 [layer_utils.py:104] INFO dense_2 (Dense)              (None, None, 1214)        311998    
2019-09-11 11:27:59,971 [layer_utils.py:169] INFO =================================================================
2019-09-11 11:27:59,972 [layer_utils.py:182] INFO Total params: 2,012,062
2019-09-11 11:27:59,973 [layer_utils.py:183] INFO Trainable params: 2,010,142
2019-09-11 11:27:59,974 [layer_utils.py:184] INFO Non-trainable params: 1,920
2019-09-11 11:27:59,974 [layer_utils.py:185] INFO _________________________________________________________________
2019-09-11 11:28:08,380 [train.py:86] INFO # of samples: 130098
2019-09-11 11:28:08,382 [train.py:87] INFO mini-batch size: 64
2019-09-11 11:28:08,383 [train.py:88] INFO # of iterations per epoch: 2032
2019-09-11 11:28:08,453 [deprecation.py:323] WARNING From /home/shaohuais/tf1.13.1/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-09-11 11:28:08,889 [deprecation.py:323] WARNING From /home/shaohuais/tf1.13.1/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
2019-09-11 11:29:18,744 [callbacks.py:16] INFO For batch 40/2032, loss is  136.64.
2019-09-11 11:29:36,505 [callbacks.py:16] INFO For batch 80/2032, loss is  103.19.
2019-09-11 11:29:53,021 [callbacks.py:16] INFO For batch 120/2032, loss is   99.39.
2019-09-11 11:30:09,608 [callbacks.py:16] INFO For batch 160/2032, loss is   98.20.
2019-09-11 11:30:23,861 [callbacks.py:16] INFO For batch 200/2032, loss is   98.36.
2019-09-11 11:30:36,852 [callbacks.py:16] INFO For batch 240/2032, loss is   98.52.
2019-09-11 11:30:52,079 [callbacks.py:16] INFO For batch 280/2032, loss is   98.50.
2019-09-11 11:31:04,514 [callbacks.py:16] INFO For batch 320/2032, loss is   97.01.
2019-09-11 11:31:17,234 [callbacks.py:16] INFO For batch 360/2032, loss is   98.26.
2019-09-11 11:31:28,330 [callbacks.py:16] INFO For batch 400/2032, loss is   97.32.
2019-09-11 11:31:39,235 [callbacks.py:16] INFO For batch 440/2032, loss is   95.80.
2019-09-11 11:31:51,261 [callbacks.py:16] INFO For batch 480/2032, loss is   95.20.
2019-09-11 11:32:03,040 [callbacks.py:16] INFO For batch 520/2032, loss is   94.27.
2019-09-11 11:32:13,426 [callbacks.py:16] INFO For batch 560/2032, loss is   93.06.
2019-09-11 11:32:24,495 [callbacks.py:16] INFO For batch 600/2032, loss is   92.10.
2019-09-11 11:32:36,536 [callbacks.py:16] INFO For batch 640/2032, loss is   90.07.
2019-09-11 11:32:49,794 [callbacks.py:16] INFO For batch 680/2032, loss is   89.60.
2019-09-11 11:33:00,103 [callbacks.py:16] INFO For batch 720/2032, loss is   86.68.
2019-09-11 11:33:11,470 [callbacks.py:16] INFO For batch 760/2032, loss is   85.10.
2019-09-11 11:33:21,948 [callbacks.py:16] INFO For batch 800/2032, loss is   81.27.
2019-09-11 11:33:34,588 [callbacks.py:16] INFO For batch 840/2032, loss is   79.96.
2019-09-11 11:33:46,744 [callbacks.py:16] INFO For batch 880/2032, loss is   77.70.
2019-09-11 11:33:57,284 [callbacks.py:16] INFO For batch 920/2032, loss is   78.57.
2019-09-11 11:34:07,260 [callbacks.py:16] INFO For batch 960/2032, loss is   75.29.
2019-09-11 11:34:17,704 [callbacks.py:16] INFO For batch 1000/2032, loss is   72.86.
2019-09-11 11:34:28,804 [callbacks.py:16] INFO For batch 1040/2032, loss is   72.83.
2019-09-11 11:34:39,382 [callbacks.py:16] INFO For batch 1080/2032, loss is   70.21.
2019-09-11 11:34:50,169 [callbacks.py:16] INFO For batch 1120/2032, loss is   68.74.
2019-09-11 11:35:01,593 [callbacks.py:16] INFO For batch 1160/2032, loss is   69.59.
2019-09-11 11:35:13,345 [callbacks.py:16] INFO For batch 1200/2032, loss is   67.88.
2019-09-11 11:35:23,713 [callbacks.py:16] INFO For batch 1240/2032, loss is   66.18.
2019-09-11 11:35:33,877 [callbacks.py:16] INFO For batch 1280/2032, loss is   64.97.
2019-09-11 11:35:45,077 [callbacks.py:16] INFO For batch 1320/2032, loss is   64.02.
2019-09-11 11:35:55,729 [callbacks.py:16] INFO For batch 1360/2032, loss is   63.67.
2019-09-11 11:36:06,947 [callbacks.py:16] INFO For batch 1400/2032, loss is   63.36.
2019-09-11 11:36:17,188 [callbacks.py:16] INFO For batch 1440/2032, loss is   62.42.
2019-09-11 11:36:26,774 [callbacks.py:16] INFO For batch 1480/2032, loss is   61.75.
2019-09-11 11:36:36,819 [callbacks.py:16] INFO For batch 1520/2032, loss is   60.64.
2019-09-11 11:36:46,554 [callbacks.py:16] INFO For batch 1560/2032, loss is   60.07.
2019-09-11 11:36:56,590 [callbacks.py:16] INFO For batch 1600/2032, loss is   60.52.
2019-09-11 11:37:05,893 [callbacks.py:16] INFO For batch 1640/2032, loss is   58.99.
2019-09-11 11:37:15,760 [callbacks.py:16] INFO For batch 1680/2032, loss is   58.67.
2019-09-11 11:37:25,182 [callbacks.py:16] INFO For batch 1720/2032, loss is   58.15.
2019-09-11 11:37:34,435 [callbacks.py:16] INFO For batch 1760/2032, loss is   56.81.
2019-09-11 11:37:43,955 [callbacks.py:16] INFO For batch 1800/2032, loss is   55.96.
2019-09-11 11:37:53,138 [callbacks.py:16] INFO For batch 1840/2032, loss is   55.86.
2019-09-11 11:38:03,763 [callbacks.py:16] INFO For batch 1880/2032, loss is   56.27.
2019-09-11 11:38:13,536 [callbacks.py:16] INFO For batch 1920/2032, loss is   56.36.
2019-09-11 11:38:23,459 [callbacks.py:16] INFO For batch 1960/2032, loss is   55.52.
2019-09-11 11:38:33,128 [callbacks.py:16] INFO For batch 2000/2032, loss is   55.56.
2019-09-11 11:39:59,800 [callbacks.py:24] INFO The validation average loss is  284.91.
2019-09-11 11:40:01,074 [callbacks.py:28] INFO The average loss for epoch 0 is   76.78.
2019-09-11 11:40:11,085 [callbacks.py:16] INFO For batch 40/2032, loss is   98.23.
2019-09-11 11:40:20,694 [callbacks.py:16] INFO For batch 80/2032, loss is   53.50.
2019-09-11 11:40:30,406 [callbacks.py:16] INFO For batch 120/2032, loss is   53.37.
2019-09-11 11:40:39,685 [callbacks.py:16] INFO For batch 160/2032, loss is   52.62.
2019-09-11 11:40:49,509 [callbacks.py:16] INFO For batch 200/2032, loss is   52.52.
2019-09-11 11:40:58,470 [callbacks.py:16] INFO For batch 240/2032, loss is   52.13.
2019-09-11 11:41:07,926 [callbacks.py:16] INFO For batch 280/2032, loss is   51.68.
2019-09-11 11:41:17,623 [callbacks.py:16] INFO For batch 320/2032, loss is   50.71.
2019-09-11 11:41:26,916 [callbacks.py:16] INFO For batch 360/2032, loss is   51.01.
2019-09-11 11:41:36,558 [callbacks.py:16] INFO For batch 400/2032, loss is   50.33.
2019-09-11 11:41:46,049 [callbacks.py:16] INFO For batch 440/2032, loss is   50.75.
2019-09-11 11:41:55,309 [callbacks.py:16] INFO For batch 480/2032, loss is   50.14.
2019-09-11 11:42:04,091 [callbacks.py:16] INFO For batch 520/2032, loss is   49.71.
2019-09-11 11:42:14,430 [callbacks.py:16] INFO For batch 560/2032, loss is   49.97.
2019-09-11 11:42:23,523 [callbacks.py:16] INFO For batch 600/2032, loss is   50.55.
2019-09-11 11:42:33,185 [callbacks.py:16] INFO For batch 640/2032, loss is   49.88.
2019-09-11 11:42:42,362 [callbacks.py:16] INFO For batch 680/2032, loss is   49.04.
2019-09-11 11:42:51,477 [callbacks.py:16] INFO For batch 720/2032, loss is   49.01.
2019-09-11 11:43:01,081 [callbacks.py:16] INFO For batch 760/2032, loss is   48.67.
2019-09-11 11:43:10,459 [callbacks.py:16] INFO For batch 800/2032, loss is   48.00.
2019-09-11 11:43:19,964 [callbacks.py:16] INFO For batch 840/2032, loss is   49.44.
2019-09-11 11:43:29,128 [callbacks.py:16] INFO For batch 880/2032, loss is   47.04.
2019-09-11 11:43:38,129 [callbacks.py:16] INFO For batch 920/2032, loss is   48.29.
2019-09-11 11:43:48,200 [callbacks.py:16] INFO For batch 960/2032, loss is   47.48.
2019-09-11 11:43:57,624 [callbacks.py:16] INFO For batch 1000/2032, loss is   47.54.
2019-09-11 11:44:07,039 [callbacks.py:16] INFO For batch 1040/2032, loss is   47.94.
2019-09-11 11:44:16,230 [callbacks.py:16] INFO For batch 1080/2032, loss is   46.86.
2019-09-11 11:44:25,737 [callbacks.py:16] INFO For batch 1120/2032, loss is   47.10.
2019-09-11 11:44:35,463 [callbacks.py:16] INFO For batch 1160/2032, loss is   46.86.
2019-09-11 11:44:45,348 [callbacks.py:16] INFO For batch 1200/2032, loss is   46.80.
2019-09-11 11:44:54,759 [callbacks.py:16] INFO For batch 1240/2032, loss is   46.07.
2019-09-11 11:45:04,117 [callbacks.py:16] INFO For batch 1280/2032, loss is   46.92.
2019-09-11 11:45:13,484 [callbacks.py:16] INFO For batch 1320/2032, loss is   45.29.
2019-09-11 11:45:22,553 [callbacks.py:16] INFO For batch 1360/2032, loss is   45.83.
2019-09-11 11:45:31,526 [callbacks.py:16] INFO For batch 1400/2032, loss is   45.84.
2019-09-11 11:45:40,805 [callbacks.py:16] INFO For batch 1440/2032, loss is   45.84.
2019-09-11 11:45:50,499 [callbacks.py:16] INFO For batch 1480/2032, loss is   45.90.
2019-09-11 11:45:59,860 [callbacks.py:16] INFO For batch 1520/2032, loss is   44.82.
2019-09-11 11:46:09,901 [callbacks.py:16] INFO For batch 1560/2032, loss is   45.69.
2019-09-11 11:46:19,598 [callbacks.py:16] INFO For batch 1600/2032, loss is   45.94.
2019-09-11 11:46:29,093 [callbacks.py:16] INFO For batch 1640/2032, loss is   45.30.
2019-09-11 11:46:38,481 [callbacks.py:16] INFO For batch 1680/2032, loss is   44.82.
2019-09-11 11:46:47,922 [callbacks.py:16] INFO For batch 1720/2032, loss is   45.38.
2019-09-11 11:46:57,495 [callbacks.py:16] INFO For batch 1760/2032, loss is   44.20.
2019-09-11 11:47:06,975 [callbacks.py:16] INFO For batch 1800/2032, loss is   44.29.
2019-09-11 11:47:16,466 [callbacks.py:16] INFO For batch 1840/2032, loss is   44.39.
2019-09-11 11:47:25,564 [callbacks.py:16] INFO For batch 1880/2032, loss is   43.95.
2019-09-11 11:47:35,193 [callbacks.py:16] INFO For batch 1920/2032, loss is   44.88.
2019-09-11 11:47:44,455 [callbacks.py:16] INFO For batch 1960/2032, loss is   44.65.
2019-09-11 11:47:53,989 [callbacks.py:16] INFO For batch 2000/2032, loss is   43.60.
2019-09-11 11:49:11,885 [callbacks.py:24] INFO The validation average loss is  301.05.
2019-09-11 11:49:11,889 [callbacks.py:28] INFO The average loss for epoch 1 is   47.88.
2019-09-11 11:49:21,276 [callbacks.py:16] INFO For batch 40/2032, loss is   79.07.
2019-09-11 11:49:30,489 [callbacks.py:16] INFO For batch 80/2032, loss is   43.93.
2019-09-11 11:49:40,031 [callbacks.py:16] INFO For batch 120/2032, loss is   43.66.
2019-09-11 11:49:49,225 [callbacks.py:16] INFO For batch 160/2032, loss is   43.69.
2019-09-11 11:49:58,338 [callbacks.py:16] INFO For batch 200/2032, loss is   42.87.
2019-09-11 11:50:07,623 [callbacks.py:16] INFO For batch 240/2032, loss is   43.71.
2019-09-11 11:50:17,284 [callbacks.py:16] INFO For batch 280/2032, loss is   43.80.
2019-09-11 11:50:26,435 [callbacks.py:16] INFO For batch 320/2032, loss is   42.85.
2019-09-11 11:50:35,462 [callbacks.py:16] INFO For batch 360/2032, loss is   42.84.
2019-09-11 11:50:44,812 [callbacks.py:16] INFO For batch 400/2032, loss is   43.45.
2019-09-11 11:50:53,983 [callbacks.py:16] INFO For batch 440/2032, loss is   43.07.
2019-09-11 11:51:03,394 [callbacks.py:16] INFO For batch 480/2032, loss is   42.37.
2019-09-11 11:51:12,667 [callbacks.py:16] INFO For batch 520/2032, loss is   43.11.
2019-09-11 11:51:21,992 [callbacks.py:16] INFO For batch 560/2032, loss is   42.96.
2019-09-11 11:51:31,479 [callbacks.py:16] INFO For batch 600/2032, loss is   42.25.
2019-09-11 11:51:40,892 [callbacks.py:16] INFO For batch 640/2032, loss is   42.87.
2019-09-11 11:51:50,419 [callbacks.py:16] INFO For batch 680/2032, loss is   42.02.
2019-09-11 11:51:59,603 [callbacks.py:16] INFO For batch 720/2032, loss is   41.46.
2019-09-11 11:52:09,186 [callbacks.py:16] INFO For batch 760/2032, loss is   41.71.
2019-09-11 11:52:18,423 [callbacks.py:16] INFO For batch 800/2032, loss is   42.68.
2019-09-11 11:52:28,152 [callbacks.py:16] INFO For batch 840/2032, loss is   42.30.
2019-09-11 11:52:37,512 [callbacks.py:16] INFO For batch 880/2032, loss is   41.54.
2019-09-11 11:52:46,715 [callbacks.py:16] INFO For batch 920/2032, loss is   41.64.
2019-09-11 11:52:55,983 [callbacks.py:16] INFO For batch 960/2032, loss is   41.83.
2019-09-11 11:53:05,886 [callbacks.py:16] INFO For batch 1000/2032, loss is   42.03.
2019-09-11 11:53:15,269 [callbacks.py:16] INFO For batch 1040/2032, loss is   41.45.
2019-09-11 11:53:24,527 [callbacks.py:16] INFO For batch 1080/2032, loss is   41.11.
2019-09-11 11:53:34,095 [callbacks.py:16] INFO For batch 1120/2032, loss is   41.43.
2019-09-11 11:53:43,533 [callbacks.py:16] INFO For batch 1160/2032, loss is   41.81.
2019-09-11 11:53:53,386 [callbacks.py:16] INFO For batch 1200/2032, loss is   41.25.
2019-09-11 11:54:02,579 [callbacks.py:16] INFO For batch 1240/2032, loss is   41.96.
2019-09-11 11:54:11,811 [callbacks.py:16] INFO For batch 1280/2032, loss is   41.25.
2019-09-11 11:54:20,896 [callbacks.py:16] INFO For batch 1320/2032, loss is   40.20.
2019-09-11 11:54:30,154 [callbacks.py:16] INFO For batch 1360/2032, loss is   41.41.
2019-09-11 11:54:39,502 [callbacks.py:16] INFO For batch 1400/2032, loss is   40.66.
2019-09-11 11:54:48,608 [callbacks.py:16] INFO For batch 1440/2032, loss is   41.00.
2019-09-11 11:54:57,557 [callbacks.py:16] INFO For batch 1480/2032, loss is   40.53.
2019-09-11 11:55:06,696 [callbacks.py:16] INFO For batch 1520/2032, loss is   40.83.
2019-09-11 11:55:16,094 [callbacks.py:16] INFO For batch 1560/2032, loss is   40.29.
2019-09-11 11:55:25,692 [callbacks.py:16] INFO For batch 1600/2032, loss is   40.35.
2019-09-11 11:55:35,423 [callbacks.py:16] INFO For batch 1640/2032, loss is   40.11.
2019-09-11 11:55:44,834 [callbacks.py:16] INFO For batch 1680/2032, loss is   39.73.
2019-09-11 11:55:53,550 [callbacks.py:16] INFO For batch 1720/2032, loss is   40.71.
2019-09-11 11:56:03,139 [callbacks.py:16] INFO For batch 1760/2032, loss is   40.76.
2019-09-11 11:56:12,569 [callbacks.py:16] INFO For batch 1800/2032, loss is   40.18.
2019-09-11 11:56:21,737 [callbacks.py:16] INFO For batch 1840/2032, loss is   40.61.
2019-09-11 11:56:31,450 [callbacks.py:16] INFO For batch 1880/2032, loss is   40.19.
2019-09-11 11:56:41,819 [callbacks.py:16] INFO For batch 1920/2032, loss is   40.75.
2019-09-11 11:56:51,161 [callbacks.py:16] INFO For batch 1960/2032, loss is   39.68.
2019-09-11 11:57:00,563 [callbacks.py:16] INFO For batch 2000/2032, loss is   38.90.
2019-09-11 11:57:53,452 [callbacks.py:24] INFO The validation average loss is  151.29.
2019-09-11 11:57:53,732 [callbacks.py:28] INFO The average loss for epoch 2 is   41.70.
2019-09-11 11:58:03,217 [callbacks.py:16] INFO For batch 40/2032, loss is   71.94.
2019-09-11 11:58:12,025 [callbacks.py:16] INFO For batch 80/2032, loss is   39.31.
2019-09-11 11:58:21,338 [callbacks.py:16] INFO For batch 120/2032, loss is   39.88.
2019-09-11 11:58:30,782 [callbacks.py:16] INFO For batch 160/2032, loss is   40.14.
2019-09-11 11:58:40,065 [callbacks.py:16] INFO For batch 200/2032, loss is   39.52.
2019-09-11 11:58:49,448 [callbacks.py:16] INFO For batch 240/2032, loss is   39.49.
2019-09-11 11:58:59,071 [callbacks.py:16] INFO For batch 280/2032, loss is   39.55.
2019-09-11 11:59:08,245 [callbacks.py:16] INFO For batch 320/2032, loss is   39.22.
2019-09-11 11:59:17,016 [callbacks.py:16] INFO For batch 360/2032, loss is   38.57.
2019-09-11 11:59:25,903 [callbacks.py:16] INFO For batch 400/2032, loss is   38.86.
2019-09-11 11:59:34,806 [callbacks.py:16] INFO For batch 440/2032, loss is   39.38.
2019-09-11 11:59:44,057 [callbacks.py:16] INFO For batch 480/2032, loss is   38.82.
2019-09-11 11:59:53,309 [callbacks.py:16] INFO For batch 520/2032, loss is   38.18.
2019-09-11 12:00:02,980 [callbacks.py:16] INFO For batch 560/2032, loss is   39.36.
2019-09-11 12:00:12,039 [callbacks.py:16] INFO For batch 600/2032, loss is   38.33.
2019-09-11 12:00:21,683 [callbacks.py:16] INFO For batch 640/2032, loss is   38.61.
2019-09-11 12:00:30,694 [callbacks.py:16] INFO For batch 680/2032, loss is   38.97.
2019-09-11 12:00:39,596 [callbacks.py:16] INFO For batch 720/2032, loss is   39.15.
2019-09-11 12:00:48,681 [callbacks.py:16] INFO For batch 760/2032, loss is   38.18.
2019-09-11 12:00:57,631 [callbacks.py:16] INFO For batch 800/2032, loss is   38.19.
2019-09-11 12:01:07,089 [callbacks.py:16] INFO For batch 840/2032, loss is   38.78.
2019-09-11 12:01:16,567 [callbacks.py:16] INFO For batch 880/2032, loss is   38.16.
2019-09-11 12:01:25,877 [callbacks.py:16] INFO For batch 920/2032, loss is   38.39.
2019-09-11 12:01:35,608 [callbacks.py:16] INFO For batch 960/2032, loss is   38.47.
2019-09-11 12:01:44,819 [callbacks.py:16] INFO For batch 1000/2032, loss is   38.28.
2019-09-11 12:01:53,852 [callbacks.py:16] INFO For batch 1040/2032, loss is   38.08.
2019-09-11 12:02:02,893 [callbacks.py:16] INFO For batch 1080/2032, loss is   38.98.
2019-09-11 12:02:11,876 [callbacks.py:16] INFO For batch 1120/2032, loss is   38.06.
2019-09-11 12:02:21,550 [callbacks.py:16] INFO For batch 1160/2032, loss is   39.31.
2019-09-11 12:02:31,308 [callbacks.py:16] INFO For batch 1200/2032, loss is   38.71.
2019-09-11 12:02:40,626 [callbacks.py:16] INFO For batch 1240/2032, loss is   37.94.
2019-09-11 12:02:50,197 [callbacks.py:16] INFO For batch 1280/2032, loss is   38.57.
2019-09-11 12:02:59,273 [callbacks.py:16] INFO For batch 1320/2032, loss is   37.04.
2019-09-11 12:03:08,388 [callbacks.py:16] INFO For batch 1360/2032, loss is   37.39.
2019-09-11 12:03:17,579 [callbacks.py:16] INFO For batch 1400/2032, loss is   37.28.
2019-09-11 12:03:26,693 [callbacks.py:16] INFO For batch 1440/2032, loss is   37.26.
2019-09-11 12:03:35,927 [callbacks.py:16] INFO For batch 1480/2032, loss is   37.10.
2019-09-11 12:03:45,676 [callbacks.py:16] INFO For batch 1520/2032, loss is   37.36.
2019-09-11 12:03:54,640 [callbacks.py:16] INFO For batch 1560/2032, loss is   38.21.
2019-09-11 12:04:04,159 [callbacks.py:16] INFO For batch 1600/2032, loss is   37.93.
2019-09-11 12:04:13,340 [callbacks.py:16] INFO For batch 1640/2032, loss is   37.74.
2019-09-11 12:04:23,234 [callbacks.py:16] INFO For batch 1680/2032, loss is   38.15.
2019-09-11 12:04:32,626 [callbacks.py:16] INFO For batch 1720/2032, loss is   36.91.
2019-09-11 12:04:41,705 [callbacks.py:16] INFO For batch 1760/2032, loss is   38.11.
2019-09-11 12:04:50,887 [callbacks.py:16] INFO For batch 1800/2032, loss is   37.52.
2019-09-11 12:05:00,512 [callbacks.py:16] INFO For batch 1840/2032, loss is   37.99.
2019-09-11 12:05:09,679 [callbacks.py:16] INFO For batch 1880/2032, loss is   37.96.
2019-09-11 12:05:19,393 [callbacks.py:16] INFO For batch 1920/2032, loss is   38.38.
2019-09-11 12:05:28,791 [callbacks.py:16] INFO For batch 1960/2032, loss is   36.98.
2019-09-11 12:05:37,937 [callbacks.py:16] INFO For batch 2000/2032, loss is   37.40.
2019-09-11 12:06:25,338 [callbacks.py:24] INFO The validation average loss is  152.65.
2019-09-11 12:06:25,342 [callbacks.py:28] INFO The average loss for epoch 3 is   38.36.
2019-09-11 12:06:35,132 [callbacks.py:16] INFO For batch 40/2032, loss is   67.04.
2019-09-11 12:06:44,860 [callbacks.py:16] INFO For batch 80/2032, loss is   36.93.
2019-09-11 12:06:54,544 [callbacks.py:16] INFO For batch 120/2032, loss is   37.37.
2019-09-11 12:07:03,874 [callbacks.py:16] INFO For batch 160/2032, loss is   36.42.
2019-09-11 12:07:13,103 [callbacks.py:16] INFO For batch 200/2032, loss is   37.84.
2019-09-11 12:07:22,736 [callbacks.py:16] INFO For batch 240/2032, loss is   37.13.
2019-09-11 12:07:32,261 [callbacks.py:16] INFO For batch 280/2032, loss is   36.48.
2019-09-11 12:07:41,670 [callbacks.py:16] INFO For batch 320/2032, loss is   35.88.
2019-09-11 12:07:50,912 [callbacks.py:16] INFO For batch 360/2032, loss is   36.54.
2019-09-11 12:08:00,525 [callbacks.py:16] INFO For batch 400/2032, loss is   37.18.
2019-09-11 12:08:09,347 [callbacks.py:16] INFO For batch 440/2032, loss is   35.89.
2019-09-11 12:08:18,555 [callbacks.py:16] INFO For batch 480/2032, loss is   36.45.
2019-09-11 12:08:28,399 [callbacks.py:16] INFO For batch 520/2032, loss is   36.98.
2019-09-11 12:08:37,637 [callbacks.py:16] INFO For batch 560/2032, loss is   37.12.
2019-09-11 12:08:47,125 [callbacks.py:16] INFO For batch 600/2032, loss is   36.57.
2019-09-11 12:08:56,794 [callbacks.py:16] INFO For batch 640/2032, loss is   36.18.
2019-09-11 12:09:05,742 [callbacks.py:16] INFO For batch 680/2032, loss is   36.48.
2019-09-11 12:09:15,586 [callbacks.py:16] INFO For batch 720/2032, loss is   36.94.
2019-09-11 12:09:24,850 [callbacks.py:16] INFO For batch 760/2032, loss is   36.74.
2019-09-11 12:09:34,220 [callbacks.py:16] INFO For batch 800/2032, loss is   36.34.
2019-09-11 12:09:43,771 [callbacks.py:16] INFO For batch 840/2032, loss is   37.36.
2019-09-11 12:09:53,294 [callbacks.py:16] INFO For batch 880/2032, loss is   35.64.
2019-09-11 12:10:02,874 [callbacks.py:16] INFO For batch 920/2032, loss is   37.22.
2019-09-11 12:10:12,400 [callbacks.py:16] INFO For batch 960/2032, loss is   36.74.
2019-09-11 12:10:22,141 [callbacks.py:16] INFO For batch 1000/2032, loss is   37.45.
2019-09-11 12:10:31,804 [callbacks.py:16] INFO For batch 1040/2032, loss is   36.60.
2019-09-11 12:10:41,453 [callbacks.py:16] INFO For batch 1080/2032, loss is   36.52.
2019-09-11 12:10:50,561 [callbacks.py:16] INFO For batch 1120/2032, loss is   36.62.
2019-09-11 12:11:00,003 [callbacks.py:16] INFO For batch 1160/2032, loss is   36.09.
2019-09-11 12:11:09,637 [callbacks.py:16] INFO For batch 1200/2032, loss is   35.95.
2019-09-11 12:11:19,259 [callbacks.py:16] INFO For batch 1240/2032, loss is   36.35.
2019-09-11 12:11:28,663 [callbacks.py:16] INFO For batch 1280/2032, loss is   37.02.
2019-09-11 12:11:38,282 [callbacks.py:16] INFO For batch 1320/2032, loss is   35.91.
2019-09-11 12:11:47,952 [callbacks.py:16] INFO For batch 1360/2032, loss is   35.83.
2019-09-11 12:11:57,903 [callbacks.py:16] INFO For batch 1400/2032, loss is   35.64.
2019-09-11 12:12:07,515 [callbacks.py:16] INFO For batch 1440/2032, loss is   35.95.
2019-09-11 12:12:17,063 [callbacks.py:16] INFO For batch 1480/2032, loss is   35.74.
2019-09-11 12:12:25,965 [callbacks.py:16] INFO For batch 1520/2032, loss is   35.38.
2019-09-11 12:12:35,298 [callbacks.py:16] INFO For batch 1560/2032, loss is   35.72.
2019-09-11 12:12:44,155 [callbacks.py:16] INFO For batch 1600/2032, loss is   36.62.
2019-09-11 12:12:53,572 [callbacks.py:16] INFO For batch 1640/2032, loss is   35.67.
2019-09-11 12:13:02,843 [callbacks.py:16] INFO For batch 1680/2032, loss is   35.99.
2019-09-11 12:13:12,415 [callbacks.py:16] INFO For batch 1720/2032, loss is   35.91.
2019-09-11 12:13:21,839 [callbacks.py:16] INFO For batch 1760/2032, loss is   36.12.
2019-09-11 12:13:31,025 [callbacks.py:16] INFO For batch 1800/2032, loss is   35.70.
2019-09-11 12:13:40,495 [callbacks.py:16] INFO For batch 1840/2032, loss is   36.12.
2019-09-11 12:13:49,577 [callbacks.py:16] INFO For batch 1880/2032, loss is   35.79.
2019-09-11 12:13:58,510 [callbacks.py:16] INFO For batch 1920/2032, loss is   35.23.
2019-09-11 12:14:07,805 [callbacks.py:16] INFO For batch 1960/2032, loss is   35.92.
2019-09-11 12:14:17,258 [callbacks.py:16] INFO For batch 2000/2032, loss is   35.23.
2019-09-11 12:14:57,635 [callbacks.py:24] INFO The validation average loss is  163.64.
2019-09-11 12:14:57,637 [callbacks.py:28] INFO The average loss for epoch 4 is   36.38.
2019-09-11 12:15:06,878 [callbacks.py:16] INFO For batch 40/2032, loss is   65.13.
2019-09-11 12:15:16,194 [callbacks.py:16] INFO For batch 80/2032, loss is   35.57.
2019-09-11 12:15:25,532 [callbacks.py:16] INFO For batch 120/2032, loss is   35.97.
2019-09-11 12:15:35,392 [callbacks.py:16] INFO For batch 160/2032, loss is   35.50.
2019-09-11 12:15:44,600 [callbacks.py:16] INFO For batch 200/2032, loss is   35.73.
2019-09-11 12:15:54,265 [callbacks.py:16] INFO For batch 240/2032, loss is   35.45.
2019-09-11 12:16:03,593 [callbacks.py:16] INFO For batch 280/2032, loss is   35.83.
2019-09-11 12:16:13,132 [callbacks.py:16] INFO For batch 320/2032, loss is   35.57.
2019-09-11 12:16:22,516 [callbacks.py:16] INFO For batch 360/2032, loss is   34.67.
2019-09-11 12:16:31,834 [callbacks.py:16] INFO For batch 400/2032, loss is   35.57.
2019-09-11 12:16:41,303 [callbacks.py:16] INFO For batch 440/2032, loss is   34.96.
2019-09-11 12:16:50,747 [callbacks.py:16] INFO For batch 480/2032, loss is   35.19.
2019-09-11 12:16:59,927 [callbacks.py:16] INFO For batch 520/2032, loss is   34.03.
2019-09-11 12:17:09,362 [callbacks.py:16] INFO For batch 560/2032, loss is   35.12.
2019-09-11 12:17:18,809 [callbacks.py:16] INFO For batch 600/2032, loss is   34.94.
2019-09-11 12:17:27,795 [callbacks.py:16] INFO For batch 640/2032, loss is   35.95.
2019-09-11 12:17:36,956 [callbacks.py:16] INFO For batch 680/2032, loss is   35.70.
2019-09-11 12:17:46,026 [callbacks.py:16] INFO For batch 720/2032, loss is   34.38.
2019-09-11 12:17:55,376 [callbacks.py:16] INFO For batch 760/2032, loss is   34.80.
2019-09-11 12:18:04,554 [callbacks.py:16] INFO For batch 800/2032, loss is   35.43.
2019-09-11 12:18:13,945 [callbacks.py:16] INFO For batch 840/2032, loss is   35.44.
2019-09-11 12:18:22,996 [callbacks.py:16] INFO For batch 880/2032, loss is   34.86.
2019-09-11 12:18:32,782 [callbacks.py:16] INFO For batch 920/2032, loss is   34.72.
2019-09-11 12:18:42,135 [callbacks.py:16] INFO For batch 960/2032, loss is   35.94.
2019-09-11 12:18:51,062 [callbacks.py:16] INFO For batch 1000/2032, loss is   34.26.
2019-09-11 12:19:00,184 [callbacks.py:16] INFO For batch 1040/2032, loss is   34.82.
2019-09-11 12:19:09,374 [callbacks.py:16] INFO For batch 1080/2032, loss is   34.56.
2019-09-11 12:19:18,747 [callbacks.py:16] INFO For batch 1120/2032, loss is   34.69.
2019-09-11 12:19:28,394 [callbacks.py:16] INFO For batch 1160/2032, loss is   34.22.
2019-09-11 12:19:38,320 [callbacks.py:16] INFO For batch 1200/2032, loss is   34.93.
2019-09-11 12:19:47,731 [callbacks.py:16] INFO For batch 1240/2032, loss is   34.52.
2019-09-11 12:19:56,581 [callbacks.py:16] INFO For batch 1280/2032, loss is   34.49.
2019-09-11 12:20:06,307 [callbacks.py:16] INFO For batch 1320/2032, loss is   35.28.
2019-09-11 12:20:15,210 [callbacks.py:16] INFO For batch 1360/2032, loss is   35.62.
2019-09-11 12:20:24,470 [callbacks.py:16] INFO For batch 1400/2032, loss is   34.99.
2019-09-11 12:20:34,287 [callbacks.py:16] INFO For batch 1440/2032, loss is   34.79.
2019-09-11 12:20:43,909 [callbacks.py:16] INFO For batch 1480/2032, loss is   34.60.
2019-09-11 12:20:53,484 [callbacks.py:16] INFO For batch 1520/2032, loss is   33.93.
2019-09-11 12:21:02,935 [callbacks.py:16] INFO For batch 1560/2032, loss is   34.61.
2019-09-11 12:21:12,544 [callbacks.py:16] INFO For batch 1600/2032, loss is   34.06.
2019-09-11 12:21:21,672 [callbacks.py:16] INFO For batch 1640/2032, loss is   35.21.
2019-09-11 12:21:31,085 [callbacks.py:16] INFO For batch 1680/2032, loss is   34.88.
2019-09-11 12:21:40,751 [callbacks.py:16] INFO For batch 1720/2032, loss is   34.37.
2019-09-11 12:21:50,526 [callbacks.py:16] INFO For batch 1760/2032, loss is   34.60.
2019-09-11 12:21:59,834 [callbacks.py:16] INFO For batch 1800/2032, loss is   34.48.
2019-09-11 12:22:09,023 [callbacks.py:16] INFO For batch 1840/2032, loss is   34.40.
2019-09-11 12:22:19,171 [callbacks.py:16] INFO For batch 1880/2032, loss is   34.51.
2019-09-11 12:22:28,768 [callbacks.py:16] INFO For batch 1920/2032, loss is   34.33.
2019-09-11 12:22:38,076 [callbacks.py:16] INFO For batch 1960/2032, loss is   34.48.
2019-09-11 12:22:47,208 [callbacks.py:16] INFO For batch 2000/2032, loss is   33.67.
2019-09-11 12:23:33,951 [callbacks.py:24] INFO The validation average loss is  164.48.
2019-09-11 12:23:33,967 [callbacks.py:28] INFO The average loss for epoch 5 is   34.92.
2019-09-11 12:23:43,669 [callbacks.py:16] INFO For batch 40/2032, loss is   61.61.
2019-09-11 12:23:53,161 [callbacks.py:16] INFO For batch 80/2032, loss is   33.67.
2019-09-11 12:24:02,752 [callbacks.py:16] INFO For batch 120/2032, loss is   34.43.
2019-09-11 12:24:12,228 [callbacks.py:16] INFO For batch 160/2032, loss is   34.09.
2019-09-11 12:24:21,663 [callbacks.py:16] INFO For batch 200/2032, loss is   34.55.
2019-09-11 12:24:30,994 [callbacks.py:16] INFO For batch 240/2032, loss is   34.40.
2019-09-11 12:24:40,681 [callbacks.py:16] INFO For batch 280/2032, loss is   34.71.
2019-09-11 12:24:50,226 [callbacks.py:16] INFO For batch 320/2032, loss is   35.00.
2019-09-11 12:24:59,194 [callbacks.py:16] INFO For batch 360/2032, loss is   34.05.
2019-09-11 12:25:08,451 [callbacks.py:16] INFO For batch 400/2032, loss is   33.91.
2019-09-11 12:25:17,540 [callbacks.py:16] INFO For batch 440/2032, loss is   34.45.
2019-09-11 12:25:26,867 [callbacks.py:16] INFO For batch 480/2032, loss is   34.34.
2019-09-11 12:25:35,920 [callbacks.py:16] INFO For batch 520/2032, loss is   34.34.
2019-09-11 12:25:45,163 [callbacks.py:16] INFO For batch 560/2032, loss is   34.58.
2019-09-11 12:25:54,109 [callbacks.py:16] INFO For batch 600/2032, loss is   34.43.
2019-09-11 12:26:03,378 [callbacks.py:16] INFO For batch 640/2032, loss is   34.08.
2019-09-11 12:26:12,971 [callbacks.py:16] INFO For batch 680/2032, loss is   34.32.
2019-09-11 12:26:22,410 [callbacks.py:16] INFO For batch 720/2032, loss is   33.29.
2019-09-11 12:26:31,803 [callbacks.py:16] INFO For batch 760/2032, loss is   33.71.
2019-09-11 12:26:41,577 [callbacks.py:16] INFO For batch 800/2032, loss is   33.97.
2019-09-11 12:26:51,048 [callbacks.py:16] INFO For batch 840/2032, loss is   34.33.
2019-09-11 12:27:00,697 [callbacks.py:16] INFO For batch 880/2032, loss is   34.36.
2019-09-11 12:27:10,267 [callbacks.py:16] INFO For batch 920/2032, loss is   33.37.
2019-09-11 12:27:19,927 [callbacks.py:16] INFO For batch 960/2032, loss is   34.29.
2019-09-11 12:27:28,814 [callbacks.py:16] INFO For batch 1000/2032, loss is   33.63.
2019-09-11 12:27:38,524 [callbacks.py:16] INFO For batch 1040/2032, loss is   34.43.
2019-09-11 12:27:47,693 [callbacks.py:16] INFO For batch 1080/2032, loss is   33.84.
2019-09-11 12:27:57,366 [callbacks.py:16] INFO For batch 1120/2032, loss is   33.50.
2019-09-11 12:28:06,572 [callbacks.py:16] INFO For batch 1160/2032, loss is   34.38.
2019-09-11 12:28:16,332 [callbacks.py:16] INFO For batch 1200/2032, loss is   34.00.
2019-09-11 12:28:25,660 [callbacks.py:16] INFO For batch 1240/2032, loss is   33.83.
2019-09-11 12:28:34,642 [callbacks.py:16] INFO For batch 1280/2032, loss is   34.19.
2019-09-11 12:28:43,805 [callbacks.py:16] INFO For batch 1320/2032, loss is   34.06.
2019-09-11 12:28:53,296 [callbacks.py:16] INFO For batch 1360/2032, loss is   33.67.
2019-09-11 12:29:02,935 [callbacks.py:16] INFO For batch 1400/2032, loss is   33.97.
2019-09-11 12:29:12,705 [callbacks.py:16] INFO For batch 1440/2032, loss is   33.07.
2019-09-11 12:29:22,178 [callbacks.py:16] INFO For batch 1480/2032, loss is   34.10.
2019-09-11 12:29:32,148 [callbacks.py:16] INFO For batch 1520/2032, loss is   33.81.
2019-09-11 12:29:41,735 [callbacks.py:16] INFO For batch 1560/2032, loss is   33.31.
2019-09-11 12:29:51,000 [callbacks.py:16] INFO For batch 1600/2032, loss is   33.58.
2019-09-11 12:30:00,258 [callbacks.py:16] INFO For batch 1640/2032, loss is   33.62.
2019-09-11 12:30:09,645 [callbacks.py:16] INFO For batch 1680/2032, loss is   32.99.
2019-09-11 12:30:19,289 [callbacks.py:16] INFO For batch 1720/2032, loss is   34.12.
2019-09-11 12:30:28,751 [callbacks.py:16] INFO For batch 1760/2032, loss is   33.66.
2019-09-11 12:30:37,781 [callbacks.py:16] INFO For batch 1800/2032, loss is   33.72.
2019-09-11 12:30:47,266 [callbacks.py:16] INFO For batch 1840/2032, loss is   33.84.
2019-09-11 12:30:56,149 [callbacks.py:16] INFO For batch 1880/2032, loss is   33.11.
2019-09-11 12:31:05,348 [callbacks.py:16] INFO For batch 1920/2032, loss is   33.22.
2019-09-11 12:31:14,341 [callbacks.py:16] INFO For batch 1960/2032, loss is   33.64.
2019-09-11 12:31:23,962 [callbacks.py:16] INFO For batch 2000/2032, loss is   33.55.
2019-09-11 12:32:07,344 [callbacks.py:24] INFO The validation average loss is  160.57.
2019-09-11 12:32:07,347 [callbacks.py:28] INFO The average loss for epoch 6 is   33.94.
2019-09-11 12:32:17,189 [callbacks.py:16] INFO For batch 40/2032, loss is   59.51.
2019-09-11 12:32:26,566 [callbacks.py:16] INFO For batch 80/2032, loss is   33.95.
2019-09-11 12:32:35,918 [callbacks.py:16] INFO For batch 120/2032, loss is   33.32.
2019-09-11 12:32:45,065 [callbacks.py:16] INFO For batch 160/2032, loss is   33.22.
2019-09-11 12:32:53,927 [callbacks.py:16] INFO For batch 200/2032, loss is   33.32.
2019-09-11 12:33:03,150 [callbacks.py:16] INFO For batch 240/2032, loss is   33.30.
2019-09-11 12:33:12,314 [callbacks.py:16] INFO For batch 280/2032, loss is   33.54.
2019-09-11 12:33:21,600 [callbacks.py:16] INFO For batch 320/2032, loss is   33.29.
2019-09-11 12:33:31,290 [callbacks.py:16] INFO For batch 360/2032, loss is   33.69.
2019-09-11 12:33:41,060 [callbacks.py:16] INFO For batch 400/2032, loss is   34.15.
2019-09-11 12:33:50,362 [callbacks.py:16] INFO For batch 440/2032, loss is   33.51.
2019-09-11 12:33:59,575 [callbacks.py:16] INFO For batch 480/2032, loss is   32.79.
2019-09-11 12:34:08,953 [callbacks.py:16] INFO For batch 520/2032, loss is   33.17.
2019-09-11 12:34:18,758 [callbacks.py:16] INFO For batch 560/2032, loss is   33.94.
2019-09-11 12:34:28,169 [callbacks.py:16] INFO For batch 600/2032, loss is   33.86.
2019-09-11 12:34:37,463 [callbacks.py:16] INFO For batch 640/2032, loss is   32.89.
2019-09-11 12:34:46,409 [callbacks.py:16] INFO For batch 680/2032, loss is   33.48.
2019-09-11 12:34:56,457 [callbacks.py:16] INFO For batch 720/2032, loss is   33.00.
2019-09-11 12:35:05,625 [callbacks.py:16] INFO For batch 760/2032, loss is   32.88.
2019-09-11 12:35:14,967 [callbacks.py:16] INFO For batch 800/2032, loss is   33.54.
2019-09-11 12:35:24,176 [callbacks.py:16] INFO For batch 840/2032, loss is   33.21.
2019-09-11 12:35:33,249 [callbacks.py:16] INFO For batch 880/2032, loss is   33.10.
2019-09-11 12:35:42,492 [callbacks.py:16] INFO For batch 920/2032, loss is   34.05.
2019-09-11 12:35:51,903 [callbacks.py:16] INFO For batch 960/2032, loss is   32.88.
2019-09-11 12:36:01,031 [callbacks.py:16] INFO For batch 1000/2032, loss is   33.20.
2019-09-11 12:36:10,296 [callbacks.py:16] INFO For batch 1040/2032, loss is   32.39.
2019-09-11 12:36:20,044 [callbacks.py:16] INFO For batch 1080/2032, loss is   33.01.
2019-09-11 12:36:29,099 [callbacks.py:16] INFO For batch 1120/2032, loss is   32.91.
2019-09-11 12:36:38,225 [callbacks.py:16] INFO For batch 1160/2032, loss is   33.85.
2019-09-11 12:36:47,248 [callbacks.py:16] INFO For batch 1200/2032, loss is   32.79.
2019-09-11 12:36:56,475 [callbacks.py:16] INFO For batch 1240/2032, loss is   32.37.
2019-09-11 12:37:05,979 [callbacks.py:16] INFO For batch 1280/2032, loss is   33.28.
2019-09-11 12:37:15,308 [callbacks.py:16] INFO For batch 1320/2032, loss is   32.53.
2019-09-11 12:37:24,149 [callbacks.py:16] INFO For batch 1360/2032, loss is   32.68.
2019-09-11 12:37:33,143 [callbacks.py:16] INFO For batch 1400/2032, loss is   33.22.
2019-09-11 12:37:42,461 [callbacks.py:16] INFO For batch 1440/2032, loss is   32.94.
2019-09-11 12:37:51,706 [callbacks.py:16] INFO For batch 1480/2032, loss is   33.24.
2019-09-11 12:38:01,037 [callbacks.py:16] INFO For batch 1520/2032, loss is   32.50.
2019-09-11 12:38:10,175 [callbacks.py:16] INFO For batch 1560/2032, loss is   32.53.
2019-09-11 12:38:19,561 [callbacks.py:16] INFO For batch 1600/2032, loss is   32.87.
2019-09-11 12:38:28,858 [callbacks.py:16] INFO For batch 1640/2032, loss is   32.77.
2019-09-11 12:38:38,161 [callbacks.py:16] INFO For batch 1680/2032, loss is   32.95.
2019-09-11 12:38:47,722 [callbacks.py:16] INFO For batch 1720/2032, loss is   32.65.
2019-09-11 12:38:57,299 [callbacks.py:16] INFO For batch 1760/2032, loss is   32.68.
2019-09-11 12:39:06,622 [callbacks.py:16] INFO For batch 1800/2032, loss is   33.01.
2019-09-11 12:39:16,158 [callbacks.py:16] INFO For batch 1840/2032, loss is   32.94.
2019-09-11 12:39:25,170 [callbacks.py:16] INFO For batch 1880/2032, loss is   32.84.
2019-09-11 12:39:34,239 [callbacks.py:16] INFO For batch 1920/2032, loss is   32.74.
2019-09-11 12:39:43,460 [callbacks.py:16] INFO For batch 1960/2032, loss is   32.00.
2019-09-11 12:39:52,826 [callbacks.py:16] INFO For batch 2000/2032, loss is   32.46.
2019-09-11 12:40:37,273 [callbacks.py:24] INFO The validation average loss is  168.37.
2019-09-11 12:40:37,275 [callbacks.py:28] INFO The average loss for epoch 7 is   33.10.
2019-09-11 12:40:46,467 [callbacks.py:16] INFO For batch 40/2032, loss is   59.42.
2019-09-11 12:40:55,911 [callbacks.py:16] INFO For batch 80/2032, loss is   32.56.
2019-09-11 12:41:05,193 [callbacks.py:16] INFO For batch 120/2032, loss is   33.18.
2019-09-11 12:41:14,325 [callbacks.py:16] INFO For batch 160/2032, loss is   32.93.
2019-09-11 12:41:23,614 [callbacks.py:16] INFO For batch 200/2032, loss is   32.48.
2019-09-11 12:41:32,642 [callbacks.py:16] INFO For batch 240/2032, loss is   33.21.
2019-09-11 12:41:41,854 [callbacks.py:16] INFO For batch 280/2032, loss is   31.78.
2019-09-11 12:41:50,969 [callbacks.py:16] INFO For batch 320/2032, loss is   33.45.
2019-09-11 12:42:00,643 [callbacks.py:16] INFO For batch 360/2032, loss is   32.25.
2019-09-11 12:42:09,761 [callbacks.py:16] INFO For batch 400/2032, loss is   32.24.
2019-09-11 12:42:19,031 [callbacks.py:16] INFO For batch 440/2032, loss is   32.94.
2019-09-11 12:42:28,078 [callbacks.py:16] INFO For batch 480/2032, loss is   31.97.
2019-09-11 12:42:37,077 [callbacks.py:16] INFO For batch 520/2032, loss is   32.02.
2019-09-11 12:42:45,987 [callbacks.py:16] INFO For batch 560/2032, loss is   32.52.
2019-09-11 12:42:55,245 [callbacks.py:16] INFO For batch 600/2032, loss is   32.21.
2019-09-11 12:43:04,684 [callbacks.py:16] INFO For batch 640/2032, loss is   32.39.
2019-09-11 12:43:14,162 [callbacks.py:16] INFO For batch 680/2032, loss is   32.34.
2019-09-11 12:43:23,522 [callbacks.py:16] INFO For batch 720/2032, loss is   32.05.
2019-09-11 12:43:33,430 [callbacks.py:16] INFO For batch 760/2032, loss is   32.45.
2019-09-11 12:43:42,852 [callbacks.py:16] INFO For batch 800/2032, loss is   33.01.
2019-09-11 12:43:52,184 [callbacks.py:16] INFO For batch 840/2032, loss is   32.57.
2019-09-11 12:44:01,691 [callbacks.py:16] INFO For batch 880/2032, loss is   32.44.
2019-09-11 12:44:10,807 [callbacks.py:16] INFO For batch 920/2032, loss is   32.58.
2019-09-11 12:44:20,386 [callbacks.py:16] INFO For batch 960/2032, loss is   32.39.
2019-09-11 12:44:29,933 [callbacks.py:16] INFO For batch 1000/2032, loss is   32.49.
2019-09-11 12:44:38,714 [callbacks.py:16] INFO For batch 1040/2032, loss is   32.05.
2019-09-11 12:44:47,845 [callbacks.py:16] INFO For batch 1080/2032, loss is   32.07.
2019-09-11 12:44:57,180 [callbacks.py:16] INFO For batch 1120/2032, loss is   32.43.
2019-09-11 12:45:06,468 [callbacks.py:16] INFO For batch 1160/2032, loss is   33.07.
2019-09-11 12:45:15,722 [callbacks.py:16] INFO For batch 1200/2032, loss is   32.50.
2019-09-11 12:45:25,161 [callbacks.py:16] INFO For batch 1240/2032, loss is   32.70.
2019-09-11 12:45:34,690 [callbacks.py:16] INFO For batch 1280/2032, loss is   32.21.
2019-09-11 12:45:43,920 [callbacks.py:16] INFO For batch 1320/2032, loss is   32.73.
2019-09-11 12:45:52,870 [callbacks.py:16] INFO For batch 1360/2032, loss is   31.63.
2019-09-11 12:46:02,344 [callbacks.py:16] INFO For batch 1400/2032, loss is   31.83.
2019-09-11 12:46:11,427 [callbacks.py:16] INFO For batch 1440/2032, loss is   32.41.
2019-09-11 12:46:20,487 [callbacks.py:16] INFO For batch 1480/2032, loss is   31.93.
2019-09-11 12:46:29,785 [callbacks.py:16] INFO For batch 1520/2032, loss is   32.80.
2019-09-11 12:46:38,557 [callbacks.py:16] INFO For batch 1560/2032, loss is   32.36.
2019-09-11 12:46:48,147 [callbacks.py:16] INFO For batch 1600/2032, loss is   32.07.
2019-09-11 12:46:57,815 [callbacks.py:16] INFO For batch 1640/2032, loss is   32.18.
2019-09-11 12:47:06,963 [callbacks.py:16] INFO For batch 1680/2032, loss is   32.71.
2019-09-11 12:47:16,392 [callbacks.py:16] INFO For batch 1720/2032, loss is   32.23.
2019-09-11 12:47:25,330 [callbacks.py:16] INFO For batch 1760/2032, loss is   31.69.
2019-09-11 12:47:34,210 [callbacks.py:16] INFO For batch 1800/2032, loss is   31.79.
2019-09-11 12:47:43,573 [callbacks.py:16] INFO For batch 1840/2032, loss is   32.77.
2019-09-11 12:47:52,688 [callbacks.py:16] INFO For batch 1880/2032, loss is   31.83.
2019-09-11 12:48:02,344 [callbacks.py:16] INFO For batch 1920/2032, loss is   32.30.
2019-09-11 12:48:11,539 [callbacks.py:16] INFO For batch 1960/2032, loss is   31.93.
2019-09-11 12:48:20,927 [callbacks.py:16] INFO For batch 2000/2032, loss is   31.91.
2019-09-11 12:49:01,949 [callbacks.py:24] INFO The validation average loss is  175.38.
2019-09-11 12:49:01,953 [callbacks.py:28] INFO The average loss for epoch 8 is   32.38.
2019-09-11 12:49:11,328 [callbacks.py:16] INFO For batch 40/2032, loss is   57.19.
2019-09-11 12:49:20,588 [callbacks.py:16] INFO For batch 80/2032, loss is   31.82.
2019-09-11 12:49:30,146 [callbacks.py:16] INFO For batch 120/2032, loss is   31.67.
2019-09-11 12:49:39,429 [callbacks.py:16] INFO For batch 160/2032, loss is   31.94.
2019-09-11 12:49:48,626 [callbacks.py:16] INFO For batch 200/2032, loss is   31.99.
2019-09-11 12:49:57,784 [callbacks.py:16] INFO For batch 240/2032, loss is   31.82.
2019-09-11 12:50:07,045 [callbacks.py:16] INFO For batch 280/2032, loss is   32.22.
2019-09-11 12:50:16,128 [callbacks.py:16] INFO For batch 320/2032, loss is   31.89.
2019-09-11 12:50:25,403 [callbacks.py:16] INFO For batch 360/2032, loss is   32.25.
2019-09-11 12:50:34,389 [callbacks.py:16] INFO For batch 400/2032, loss is   31.33.
2019-09-11 12:50:43,643 [callbacks.py:16] INFO For batch 440/2032, loss is   32.40.
2019-09-11 12:50:53,455 [callbacks.py:16] INFO For batch 480/2032, loss is   32.48.
2019-09-11 12:51:02,644 [callbacks.py:16] INFO For batch 520/2032, loss is   31.04.
2019-09-11 12:51:11,759 [callbacks.py:16] INFO For batch 560/2032, loss is   31.96.
